{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04fdda9d-0c4a-48dd-ab48-59c75ab0917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st \n",
    "import altair as alt \n",
    "import requests\n",
    "import string\n",
    "import re\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581b69f9-f484-4e4f-9ce0-c6b076178bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094b395-8def-4932-8b25-2c58a95af554",
   "metadata": {},
   "source": [
    "## Get the most ask questions' tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8633ce-2aed-4a4a-a567-063146dab58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://stackoverflow.com/tags?page=1&tab=popular'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "posts_infos = soup.find_all('div', {'class':\"mt-auto d-flex jc-space-between fs-caption fc-black-400\"})\n",
    "tags = soup.find_all('a',{'class':'post-tag'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "753cd4c5-4f2f-4bb2-bebd-87d1c5253a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_lst = []\n",
    "tags_lst = []\n",
    "dic_ask = {'tag':[],'question':[],'asked today':[],'question this week':[]}\n",
    "\n",
    "for post_infos in posts_infos:\n",
    "    post_lst.append(post_infos.text)   \n",
    "for tag in tags:\n",
    "    tags_lst.append(tag.text)\n",
    "# replace useless strings\n",
    "for j in range(len(post_lst)) :\n",
    "    post_lst[j] = post_lst[j].replace('\\n','')\n",
    "    post_lst[j] = post_lst[j].replace(' questions ',',')\n",
    "    post_lst[j] = post_lst[j].replace(' asked today, ',',')\n",
    "    post_lst[j] = post_lst[j].replace(' this week ','')\n",
    "    post_lst[j] = post_lst[j].replace(' asked this week','')\n",
    "    post_lst[j] = post_lst[j].replace(' this month ','')\n",
    "    post_lst[j] = post_lst[j].split(',')   \n",
    "# put each number into different key (ask this week,ask today and total ask)\n",
    "for i in post_lst :\n",
    "    dic_ask['question'].append(i[0])\n",
    "    dic_ask['asked today'].append(i[1])\n",
    "    dic_ask['question this week'].append(i[2])   \n",
    "    \n",
    "for i in tags:\n",
    "    dic_ask['tag'].append(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d41fd1c-bba5-479b-bc2b-ba627b5ef386",
   "metadata": {},
   "source": [
    "## Change string into num and create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e0d3e00-c13f-4eac-8225-bfb260dc31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_ask['question'] = pd.to_numeric(dic_ask['question'])\n",
    "dic_ask['asked today'] = pd.to_numeric(dic_ask['asked today'])\n",
    "dic_ask['question this week'] = pd.to_numeric(dic_ask['question this week'])\n",
    "df_ask = pd.DataFrame(dic_ask,columns = ['tag','question', 'asked today', 'question this week'])\n",
    "df_ask = df_ask.sort_values('question this week', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "094a1913-c187-4e7f-a537-12b1e119de44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_title = '<p style=\"font-family:sans-serif; color:Green; font-size: 42px;\">StackOverflow Data</p>'\n",
    "st.markdown(big_title, unsafe_allow_html=True)\n",
    "expander_bar = st.expander('About')\n",
    "expander_bar.markdown(\"\"\"\n",
    "**data source :** [stackoverflow](https://stackoverflow.com/)\"\"\")\n",
    "\n",
    "chart = alt.Chart(df_ask).mark_circle().encode(\n",
    "    x = 'asked today',\n",
    "    y = 'question this week',\n",
    "    color = 'tag',\n",
    ").interactive()\n",
    "chart2 = alt.Chart(df_ask).mark_circle().encode(\n",
    "    x = 'asked today',\n",
    "    y = 'question',\n",
    "    color = 'tag',\n",
    ").interactive()\n",
    "\n",
    "st.sidebar.header('User Input Features')\n",
    "selectcolumn = st.sidebar.selectbox('ASK of every tags today/this week/total', ['question', 'asked today', 'question this week'])\n",
    "st.subheader('dataframe & chart')\n",
    "p = alt.Chart(df_ask).mark_bar().encode(\n",
    "    x = alt.X('tag', sort=None),\n",
    "    y = selectcolumn,\n",
    ")\n",
    "p = p.properties(\n",
    "    width = alt.Step(20)\n",
    ")\n",
    "\n",
    "col1, col2 = st.columns((2,1))\n",
    "cold, cold2 = st.columns((1,1))\n",
    "# col1.write(df_ask)\n",
    "# col2.write(p)\n",
    "# cold.write(chart)\n",
    "# cold2.write(chart2)\n",
    "\n",
    "predict_title = '<p style=\"font-family:sans-serif; color:Green; font-size: 42px;\">Text Classification Using sklearn</p>'\n",
    "st.markdown(predict_title, unsafe_allow_html = True)\n",
    "#st.title('Text Classification Using sklearn')\n",
    "st.subheader('Is the question about java ? (yes : 1 , no : 0) & Java tag bar chart')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab4951-8e96-410b-94c2-a2732629c3a1",
   "metadata": {},
   "source": [
    "## Define a function that returns a data with texts and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4ae71a3-8000-4daa-bd47-b3a0677be513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_mining(page) :\n",
    "    url = 'https://stackoverflow.com/questions?tab=newest&page=' + str(1)\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "    post_titles = soup.find_all('a', {'class' : \"s-link\"})\n",
    "    hot_network_question = soup.find_all('a', {'href' : \"https://stackexchange.com/questions?tab=hot\"})\n",
    "    tags_in_posts = soup.find_all('ul', {'class' : \"ml0 list-ls-none js-post-tag-list-wrapper d-inline\"})\n",
    "\n",
    "    ml = {}\n",
    "    specific = []\n",
    "    l = 0\n",
    "    for tags_in_post in tags_in_posts :\n",
    "        if 'java' in tags_in_post.text:\n",
    "            specific.append(1)\n",
    "        else:\n",
    "            specific.append(0)\n",
    "    # confirm it's not belongs to the row 'Hot Network Questions'\n",
    "    for title in post_titles[len(post_titles) - len(tags_in_posts) - 1 : -1] :\n",
    "        if hot_network_question[0].text != title.text :\n",
    "            ml[title.text] = specific[l]\n",
    "            l += 1\n",
    "    return ml\n",
    "# remove specific characters, drop numbers, remove stopwords\n",
    "def remove_special_characters(text):\n",
    "    pat = r'[^a-zA-z0-9]' \n",
    "    return re.sub(pat, ' ', text)\n",
    "\n",
    "def drop_numbers(list_text):\n",
    "    list_text_new = []\n",
    "    for i in list_text:\n",
    "        if not re.search('\\d', i ):\n",
    "            list_text_new.append(i)\n",
    "    return ''.join(list_text_new)\n",
    "\n",
    "def remove_stopwords(text) :\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6589b310-9502-40d5-9f88-f67b883c73f7",
   "metadata": {},
   "source": [
    "## Company informations process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3635fd8-88ce-48a9-afda-55232510b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def company(page) :\n",
    "    url_companies = 'https://stackoverflow.com/jobs/companies?pg=' + str(page)\n",
    "    res_comp = requests.get(url_companies)\n",
    "    soup_comp = BeautifulSoup(res_comp.text, \"html.parser\")\n",
    "    company_name = soup_comp.find_all('a', {'class' : \"s-link\"})\n",
    "    company_loc_indus = soup_comp.find_all('div', {'class' : \"flex--item fc-black-500 fs-body1\"})\n",
    "    company_tag = soup_comp.find_all('a', {'class' : \"flex--item s-tag no-tag-menu\"})\n",
    "    page_limit_company = soup_comp.find_all('div', {'class' : \"dismissable-company -company ps-relative js-dismiss-overlay-container p24 pr32 bb bc-black-100\"})\n",
    "    company_name_lst, company_tag_lst, comp_location_industry, comp_location, comp_industry = [], [], [], [], []\n",
    "    dic_company = {}\n",
    "    #process company name\n",
    "    for name in company_name[2: ] :\n",
    "        company_name_lst.append(name.text)\n",
    "    #process company text\n",
    "    for tags in company_tag :\n",
    "        company_tag_lst.append(tags.text)\n",
    "    #process company industry and location\n",
    "    for i in company_loc_indus :\n",
    "        comp_location_industry.append(i.text)\n",
    "    for i in range(len(company_loc_indus)):\n",
    "        #comp_location.append(comp_location_industry[i]) if i%2==0 else comp_industry.append(comp_location_industry[i])\n",
    "        if i % 2 == 0:\n",
    "            comp_location.append(comp_location_industry[i])\n",
    "        else:\n",
    "            comp_industry.append(comp_location_industry[i])\n",
    "    #process each pages data  \n",
    "    for m in range(len(page_limit_company)):\n",
    "        dic_company[company_name_lst[m]] = company_tag_lst[0+3*m : 3+3*m]\n",
    "        \n",
    "    return dic_company, company_tag_lst, comp_location, comp_industry\n",
    "\n",
    "def to_df() :\n",
    "    #turn array and dictionary into dataframe\n",
    "    company_tag_freq = {}\n",
    "    #process company,tags,location,industry columns\n",
    "    df_company_and_tags = pd.DataFrame(columns = ['company', 'tags'])\n",
    "    df_loc_indus = pd.DataFrame()\n",
    "    for i in range(1, 16) :\n",
    "        #write all tag into dic , and get the frequency of tag\n",
    "        comp = company(i)\n",
    "        for tag in comp[1] :\n",
    "            company_tag_freq[tag] = 1 if tag not in company_tag_freq else company_tag_freq[tag] + 1\n",
    "        #each companys' tags\n",
    "        company_and_tags = comp[0]\n",
    "        cat = pd.DataFrame(company_and_tags.items(), columns = ['company', 'tags'])\n",
    "        lai = pd.DataFrame(comp[2:]).T\n",
    "        df_company_and_tags = pd.concat([df_company_and_tags,cat])\n",
    "        df_loc_indus = pd.concat([df_loc_indus,lai])\n",
    "    data = pd.concat([df_company_and_tags,df_loc_indus],axis = 1)\n",
    "    data.columns = ['company', 'tags', 'location', 'industry']\n",
    "    \n",
    "    return data, company_tag_freq\n",
    "\n",
    "def clean(doc) :\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punct_free = ''.join(ch for ch in stop_free if ch not in punct_exclude)\n",
    "    num_free = ''.join(i for i in punct_free if not i.isdigit())\n",
    "    return num_free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "161cef34-2104-42c9-bc46-f4ec85d4a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_page = st.sidebar.slider('page of questions for machine learning ', 2, 10, 6)\n",
    "data_total = []\n",
    "for slide in range(1, slide_page) :\n",
    "    text_data = text_mining(slide)\n",
    "    data_total.append(text_data)\n",
    "    \n",
    "data_ml = pd.concat(pd.DataFrame(data_total[i].items()) for i in range(len(data_total)))\n",
    "data_ml.columns = ['text', 'java_tag']\n",
    "# process data_ml, keep a copy \n",
    "data_copy = data_ml\n",
    "# tag count\n",
    "tag_count = pd.DataFrame(data_ml['java_tag'].value_counts())\n",
    "# st.write(tag_count)\n",
    "tag_count.index.name = 'tag'\n",
    "tag_count.reset_index(inplace = True)\n",
    "\n",
    "col3, col4 = st.columns((5, 1))\n",
    "col3.write(data_ml.head(30))\n",
    "with col4:\n",
    "    st.bar_chart(data = tag_count, x = 'tag', y = 'count', use_container_width = True)\n",
    "\n",
    "# filter text\n",
    "data_ml['text'] = data_ml.apply(lambda x: remove_special_characters(x['text']), axis = 1)\n",
    "data_ml['text'] = data_ml['text'].apply(drop_numbers)\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "data_ml[\"text\"] =data_ml[\"text\"].map(remove_stopwords)\n",
    "\n",
    "# creating bag of words\n",
    "cv = CountVectorizer(max_features = 2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2904e68-e853-43af-a6ba-2c33aa02e3ea",
   "metadata": {},
   "source": [
    "## Sentence prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b3865fb-7bea-458a-a42c-9679127f4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = cv.fit_transform(data_ml['text']).toarray()\n",
    "y = data_ml['java_tag'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0,shuffle = False)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "pred = classifier.predict(X_test)\n",
    "\n",
    "#process the predict sentences\n",
    "train_test_target = y_train.tolist()+pred.tolist()\n",
    "train_test_target = pd.DataFrame(train_test_target)\n",
    "data_copy.reset_index(drop = True, inplace = True)\n",
    "train_test_target.reset_index(drop=True, inplace = True)\n",
    "new_data_ml = pd.concat([data_copy,train_test_target],axis = 1)\n",
    "pred_sentence = []\n",
    "\n",
    "for i in range(round(0.75 * len(new_data_ml)) - 1, len(new_data_ml)) :\n",
    "    if new_data_ml[0][i] == 1 :\n",
    "       pred_sentence.append(new_data_ml['text'][i])\n",
    "    \n",
    "frame_pred = pd.DataFrame(pred_sentence)\n",
    "frame_pred.columns = ['Predict Sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3acfc662-6105-4586-b85c-972dd15fec9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.subheader('Data training and test')\n",
    "st.write('training score :', classifier.score(X_train, y_train),'test score :', classifier.score(X_test, y_test))\n",
    "st.write(\"The test score is: \", round(accuracy_score(y_test, pred) * 100,2), '%')\n",
    "acc_dic = classification_report(y_test, pred, output_dict = True)\n",
    "acc_df = pd.DataFrame(acc_dic).transpose()\n",
    "st.write(acc_df)\n",
    "st.subheader('Sentences predicted have a java tag :sunglasses:')\n",
    "st.write(frame_pred)\n",
    "st.subheader(\"Companys' news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c1fdfa1-1d0d-4c86-b0e1-ca4e76a86837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "company_data = to_df()\n",
    "#plot each tags' frequency\n",
    "company_tag = pd.DataFrame(company_data[1].items(),columns = ['tagged', 'freq'])\n",
    "company_tag = company_tag.sort_values('freq',ascending = False)\n",
    "new_company_tag = company_tag[company_tag.freq > 2]\n",
    "comp_plot = alt.Chart(new_company_tag).mark_bar().encode(\n",
    "    x = alt.X('tagged', sort = None),\n",
    "    y = 'freq',\n",
    ")\n",
    "comp_plot = comp_plot.properties(\n",
    "    width = alt.Step(20)\n",
    ")\n",
    "st.write(comp_plot) \n",
    "st.write(company_data[0])\n",
    "st.subheader('Most count words in new questions')\n",
    "\n",
    "m = 0\n",
    "dic = {}\n",
    "for i in range(10) :\n",
    "    url = 'https://stackoverflow.com/questions?tab=newest&page=' + str(i)\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "    posts_title = soup.find_all('a', {'class':\"s-link\"})\n",
    "    for title in posts_title[2:] :\n",
    "        dic[m] = title.text\n",
    "        m += 1\n",
    "        \n",
    "df = pd.DataFrame(dic.items())\n",
    "stop = set(stopwords.words())\n",
    "punct_exclude = set(string.punctuation)\n",
    "\n",
    "post_corpus = [clean(df.iloc[i, 1]) for i in range(0, df.shape[0])]\n",
    "wordcloud = WordCloud(width = 1000, height = 500, stopwords = STOPWORDS, background_color = 'white', font_path = 'RadiantKingdom-mL5eV.ttf').generate(''.join(post_corpus))\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.imshow(wordcloud)\n",
    "st.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "st.pyplot()\n",
    "#streamlit run /Users/wangqiqian/Desktop/分析軟體/UI.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fffbad-5956-4a72-95b3-2e05c7837ef8",
   "metadata": {},
   "source": [
    "## Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d91184a2-ecea-4a94-8301-128b583a86a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the most ask questions' tag\n",
    "url='https://stackoverflow.com/tags?page=1&tab=popular'\n",
    "res=requests.get(url)\n",
    "soup=BeautifulSoup(res.text,'html.parser')\n",
    "a=soup.find_all('div', {'class':\"mt-auto d-flex jc-space-between fs-caption fc-black-400\"})\n",
    "bb=soup.find_all('a',{'class':'post-tag'})\n",
    "lst_for_ask=[]\n",
    "tags=[]\n",
    "dic_ask={'tag':[],'question':[],'asked today':[],'question this week':[]}\n",
    "for i in a:\n",
    "    lst_for_ask.append(i.text)   \n",
    "for i in bb:\n",
    "    tags.append(i.text)  \n",
    "for j in range(len(lst_for_ask)):#replace useless strings\n",
    "    lst_for_ask[j]=lst_for_ask[j].replace('\\n','')\n",
    "    lst_for_ask[j]=lst_for_ask[j].replace(' questions ',',')\n",
    "    lst_for_ask[j]=lst_for_ask[j].replace(' asked today, ',',')\n",
    "    lst_for_ask[j]=lst_for_ask[j].replace(' this week ','')\n",
    "    lst_for_ask[j]=lst_for_ask[j].replace(' asked this week','')\n",
    "    lst_for_ask[j]=lst_for_ask[j].replace(' this month ','')\n",
    "    lst_for_ask[j]=lst_for_ask[j].split(',')   \n",
    "for i in lst_for_ask:#put each number into different key (ask this week,ask today and total ask)\n",
    "    dic_ask['question'].append(i[0])\n",
    "    dic_ask['asked today'].append(i[1])\n",
    "    dic_ask['question this week'].append(i[2])   \n",
    "for i in tags:\n",
    "    dic_ask['tag'].append(i)\n",
    "    \n",
    "url = 'https://stackoverflow.com/tags?page=1&tab=popular'\n",
    "res = requests.get(url)\n",
    "soup = BeautifulSoup(res.text,'html.parser')\n",
    "posts_infos = soup.find_all('div', {'class':\"mt-auto d-flex jc-space-between fs-caption fc-black-400\"})\n",
    "tags = soup.find_all('a',{'class':'post-tag'})\n",
    "\n",
    "post_lst = []\n",
    "tags_lst = []\n",
    "dic_ask = {'tag':[],'question':[],'asked today':[],'question this week':[]}\n",
    "\n",
    "for post_infos in posts_infos:\n",
    "    post_lst.append(post_infos.text)   \n",
    "for tag in tags:\n",
    "    tags_lst.append(tag.text)\n",
    "# replace useless strings\n",
    "for j in range(len(post_lst)) :\n",
    "    post_lst[j] = post_lst[j].replace('\\n','')\n",
    "    post_lst[j] = post_lst[j].replace(' questions ',',')\n",
    "    post_lst[j] = post_lst[j].replace(' asked today, ',',')\n",
    "    post_lst[j] = post_lst[j].replace(' this week ','')\n",
    "    post_lst[j] = post_lst[j].replace(' asked this week','')\n",
    "    post_lst[j] = post_lst[j].replace(' this month ','')\n",
    "    post_lst[j] = post_lst[j].split(',')   \n",
    "# put each number into different key (ask this week,ask today and total ask)\n",
    "for i in lst_for_ask :\n",
    "    dic_ask['question'].append(i[0])\n",
    "    dic_ask['asked today'].append(i[1])\n",
    "    dic_ask['question this week'].append(i[2])   \n",
    "    \n",
    "for i in tags:\n",
    "    dic_ask['tag'].append(i)\n",
    "    \n",
    "dic_ask['question'] = pd.to_numeric(dic_ask['question'])\n",
    "dic_ask['asked today'] = pd.to_numeric(dic_ask['asked today'])\n",
    "dic_ask['question this week'] = pd.to_numeric(dic_ask['question this week'])\n",
    "df_ask = pd.DataFrame(dic_ask,columns = ['tag','question', 'asked today', 'question this week'])\n",
    "df_ask = df_ask.sort_values('question this week', ascending = False)\n",
    "\n",
    "big_title = '<p style=\"font-family:sans-serif; color:Green; font-size: 42px;\">StackOverflow Data</p>'\n",
    "st.markdown(big_title, unsafe_allow_html=True)\n",
    "expander_bar = st.expander('About')\n",
    "expander_bar.markdown(\"\"\"\n",
    "**data source :** [stackoverflow](https://stackoverflow.com/)\"\"\")\n",
    "\n",
    "chart = alt.Chart(df_ask).mark_circle().encode(\n",
    "    x = 'asked today',\n",
    "    y = 'question this week',\n",
    "    color = 'tag',\n",
    ").interactive()\n",
    "chart2 = alt.Chart(df_ask).mark_circle().encode(\n",
    "    x = 'asked today',\n",
    "    y = 'question',\n",
    "    color = 'tag',\n",
    ").interactive()\n",
    "\n",
    "st.sidebar.header('User Input Features')\n",
    "selectcolumn = st.sidebar.selectbox('ASK of every tags today/this week/total', ['question', 'asked today', 'question this week'])\n",
    "st.subheader('dataframe & chart')\n",
    "p = alt.Chart(df_ask).mark_bar().encode(\n",
    "    x = alt.X('tag', sort=None),\n",
    "    y = selectcolumn,\n",
    ")\n",
    "p = p.properties(\n",
    "    width = alt.Step(20)\n",
    ")\n",
    "\n",
    "col1, col2 = st.columns((2,1))\n",
    "cold, cold2 = st.columns((1,1))\n",
    "# col1.write(df_ask)\n",
    "# col2.write(p)\n",
    "# cold.write(chart)\n",
    "# cold2.write(chart2)\n",
    "\n",
    "predict_title = '<p style=\"font-family:sans-serif; color:Green; font-size: 42px;\">Text Classification Using sklearn</p>'\n",
    "st.markdown(predict_title, unsafe_allow_html = True)\n",
    "#st.title('Text Classification Using sklearn')\n",
    "st.subheader('Is the question about java ? (yes : 1 , no : 0) & Java tag bar chart')\n",
    "\n",
    "slide_page = st.sidebar.slider('page of questions for machine learning ', 2, 10, 6)\n",
    "data_total = []\n",
    "for slide in range(1, slide_page) :\n",
    "    text_data = text_mining(slide)\n",
    "    data_total.append(text_data)\n",
    "    \n",
    "data_ml = pd.concat(pd.DataFrame(data_total[i].items()) for i in range(len(data_total)))\n",
    "data_ml.columns = ['text', 'java_tag']\n",
    "# process data_ml, keep a copy \n",
    "data_copy = data_ml\n",
    "# tag count\n",
    "tag_count = pd.DataFrame(data_ml['java_tag'].value_counts())\n",
    "# st.write(tag_count)\n",
    "tag_count.index.name = 'tag'\n",
    "tag_count.reset_index(inplace = True)\n",
    "\n",
    "col3, col4 = st.columns((5, 1))\n",
    "col3.write(data_ml.head(30))\n",
    "with col4:\n",
    "    st.bar_chart(data = tag_count, x = 'tag', y = 'count', use_container_width = True)\n",
    "\n",
    "# filter text\n",
    "data_ml['text'] = data_ml.apply(lambda x: remove_special_characters(x['text']), axis = 1)\n",
    "data_ml['text'] = data_ml['text'].apply(drop_numbers)\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "data_ml[\"text\"] =data_ml[\"text\"].map(remove_stopwords)\n",
    "\n",
    "# creating bag of words\n",
    "cv = CountVectorizer(max_features = 2500)\n",
    "\n",
    "x = cv.fit_transform(data_ml['text']).toarray()\n",
    "y = data_ml['java_tag'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 0,shuffle = False)\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "pred = classifier.predict(X_test)\n",
    "\n",
    "#process the predict sentences\n",
    "train_test_target = y_train.tolist()+pred.tolist()\n",
    "train_test_target = pd.DataFrame(train_test_target)\n",
    "data_copy.reset_index(drop = True, inplace = True)\n",
    "train_test_target.reset_index(drop=True, inplace = True)\n",
    "new_data_ml = pd.concat([data_copy,train_test_target],axis = 1)\n",
    "pred_sentence = []\n",
    "\n",
    "for i in range(round(0.75 * len(new_data_ml)) - 1, len(new_data_ml)) :\n",
    "    if new_data_ml[0][i] == 1 :\n",
    "       pred_sentence.append(new_data_ml['text'][i])\n",
    "    \n",
    "frame_pred = pd.DataFrame(pred_sentence)\n",
    "frame_pred.columns = ['Predict Sentences']\n",
    "\n",
    "st.subheader('Data training and test')\n",
    "st.write('training score :', classifier.score(X_train, y_train),'test score :', classifier.score(X_test, y_test))\n",
    "st.write(\"The test score is: \", round(accuracy_score(y_test, pred) * 100,2), '%')\n",
    "acc_dic = classification_report(y_test, pred, output_dict = True)\n",
    "acc_df = pd.DataFrame(acc_dic).transpose()\n",
    "st.write(acc_df)\n",
    "st.subheader('Sentences predicted have a java tag :sunglasses:')\n",
    "st.write(frame_pred)\n",
    "st.subheader(\"Companys' news\")\n",
    "\n",
    "company_data = to_df()\n",
    "#plot each tags' frequency\n",
    "company_tag = pd.DataFrame(company_data[1].items(),columns = ['tagged', 'freq'])\n",
    "company_tag = company_tag.sort_values('freq',ascending = False)\n",
    "new_company_tag = company_tag[company_tag.freq > 2]\n",
    "comp_plot = alt.Chart(new_company_tag).mark_bar().encode(\n",
    "    x = alt.X('tagged', sort = None),\n",
    "    y = 'freq',\n",
    ")\n",
    "comp_plot = comp_plot.properties(\n",
    "    width = alt.Step(20)\n",
    ")\n",
    "st.write(comp_plot) \n",
    "st.write(company_data[0])\n",
    "st.subheader('Most count words in new questions')\n",
    "\n",
    "m = 0\n",
    "dic = {}\n",
    "for i in range(10) :\n",
    "    url = 'https://stackoverflow.com/questions?tab=newest&page=' + str(i)\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "    posts_title = soup.find_all('a', {'class':\"s-link\"})\n",
    "    for title in posts_title[2:] :\n",
    "        dic[m] = title.text\n",
    "        m += 1\n",
    "        \n",
    "df = pd.DataFrame(dic.items())\n",
    "stop = set(stopwords.words())\n",
    "punct_exclude = set(string.punctuation)\n",
    "\n",
    "post_corpus = [clean(df.iloc[i, 1]) for i in range(0, df.shape[0])]\n",
    "wordcloud = WordCloud(width = 1000, height = 500, stopwords = STOPWORDS, background_color = 'white', font_path = 'RadiantKingdom-mL5eV.ttf').generate(''.join(post_corpus))\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.imshow(wordcloud)\n",
    "st.set_option('deprecation.showPyplotGlobalUse', False)\n",
    "st.pyplot()\n",
    "#streamlit run /Users/wangqiqian/Desktop/分析軟體/UI.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
